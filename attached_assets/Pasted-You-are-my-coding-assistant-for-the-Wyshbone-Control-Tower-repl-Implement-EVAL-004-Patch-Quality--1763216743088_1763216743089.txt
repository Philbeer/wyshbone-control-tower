You are my coding assistant for the Wyshbone Control Tower repl.
Implement EVAL-004: Patch Quality + Regression Protection (Mode C: Strict Gatekeeper).

Your task is to turn Tower into a strict CI/CD gatekeeper that evaluates patches generated by Replit Agents before they are applied.
Tower must reject any patch that causes regressions, errors, failed tests, longer latency, or lower quality.
This is a full implementation task: produce complete code, not instructions.

ðŸ”§ ARCHITECTURE TO FOLLOW

Tower currently includes:

Behaviour tests (EVAL-002)

Automatic detection + auto-investigations (EVAL-003)

Investigator diagnosis pipeline (EVAL-001)

Run logger and DB tables for runs and investigations

EVAL-004 adds a Patch Evaluation Pipeline:

Incoming patch â†’ Sandbox apply â†’ Full evaluation â†’ Gate decision â†’ Report back


You must create this pipeline inside Tower.

ðŸ“ NEW MODULES AND REQUIRED FILES

Create these new files exactly:

1. src/evaluator/patchEvaluator.ts

The central patch evaluation engine.

2. src/evaluator/patchSandbox.ts

Applies a patch in an isolated environment (in-memory file overlay).

3. src/evaluator/patchDiff.ts

Normalises and diffs pre-patch vs post-patch behaviour test results.

4. src/evaluator/patchGate.ts

Implements Mode C strict approval rules.

5. server/routes-patch.ts

New API endpoints for:

submitting a patch

checking patch status

fetching patch reports

Modify server/routes.ts to mount this module.

6. DB migration (if needed)

Add a table:

patch_evaluations
- id
- status ("pending" | "approved" | "rejected")
- createdAt
- patchText
- diff
- reasons[]
- testResultsBefore
- testResultsAfter
- investigationIds[]


If a table already exists from EVAL-001/002/003, adapt accordingly.

ðŸš¦ STRICT MODE C DECISION RULES

A patch must be REJECTED if ANY of the following occur:

âŒ 1. Any behaviour test FAILS after applying the patch

Even if it passed before.

âŒ 2. Any new ERROR appears

Including timeouts, Zod errors, 500s, or missing tools.

âŒ 3. Latency regression

If any testâ€™s durationMs increases by > 30%.

âŒ 4. Quality degradation

Use Tower's investigator result. Reject patch if:

investigator flags â€œquality dropâ€

output becomes shorter, emptier, or more generic

missing key behaviours (greeting, adaptation, search confirmation, monitoring language)

semantic degradation according to heuristics

âŒ 5. Regression detection

If previous run was PASS and patched run is FAIL.

âŒ 6. Investigator â€œdanger flagâ€

If the investigator pipeline assigns:

risk level = â€œhighâ€

or classification = "dangerous", "structurally breaking", or "major regression"

âŒ 7. Any auto-detection trigger fires

(EVAL-003 triggers MUST block patch automatically)

repeated errors

timeout

empty output

nonsense output

âŒ 8. Test suite instability

If patched run behaves inconsistently between retries.

âŒ 9. Patch is irrelevant or does nothing

(E.g., patch modifies files not used by UI/Supervisor/Tower)

âŒ 10. Patch breaks imports, exports, or server boot
ðŸŸ© APPROVAL RULES (RARE)

A patch may only be APPROVED if ALL conditions hold:

All tests PASS

No new errors

No regressions

Latency stable or improved

Quality improved or equal

Investigator marks patch as:

â€œsafeâ€

â€œbeneficialâ€

â€œaligned with behaviour specâ€

No EVAL-003 triggers

Patch modifies relevant functional code

No schema breaks

No tool-call regressions

ðŸ” WHAT THE NEW PIPELINE MUST DO

Receive a proposed code patch (full diff text)
via new endpoint:

POST /tower/patch/submit
{
  "patch": string
}


Save patch to DB

Apply patch in a sandbox

Never touch the real filesystem

Use in-memory overlay

Support multi-file diffs

Run full behaviour test suite BEFORE the patch
Store results as before.

Run full behaviour test suite AFTER the patch
Through sandboxed code loader.

Collect and normalise results
Diff them.

Run investigator on the diff
Ask investigator: â€œDid quality improve or decrease?â€

Run the EVAL-003 auto-detection logic on after-results
If any trigger fires:

mark patch REJECTED

attach investigation IDs

Evaluate patch using Mode C rules
Use patchGate.ts and return:

{
  status: "approved" | "rejected",
  reasons: [...],
  diff,
  beforeResults,
  afterResults
}


Store final decision

Return results to caller
Endpoint:

GET /tower/patch/:id


Return:

patch

decision

list of violations

investigator summary

diff summary

before/after comparison

triggered investigations

ðŸ“ˆ PERFORMANCE REQUIREMENTS

Behaviour tests must stream as normal

Sandbox must NOT slow down Tower > 25%

No test can exceed 10 seconds

All evaluation must run within 60 seconds

If needed, implement simple throttling so only one patch evaluation runs at a time.

ðŸ§ª ACCEPTANCE CRITERIA

Submitting a patch triggers:

sandbox apply

before tests

after tests

investigator

auto-detection

diff analysis

strict gate decision

A broken patch is REJECTED with reasons.

A safe patch is APPROVED.

All logic is deterministic.

No UI changes required.

All new endpoints are reachable and documented.

No interference with existing EVAL-001â€’003 behaviour.

All code is production-ready.

ðŸ“„ FINAL OUTPUT REQUIREMENT

Agent must output full file contents for:

src/evaluator/patchEvaluator.ts

src/evaluator/patchSandbox.ts

src/evaluator/patchGate.ts

src/evaluator/patchDiff.ts

server/routes-patch.ts

DB migration (if needed)

AND diffs for each modified file.

No explanations.
Only full code.